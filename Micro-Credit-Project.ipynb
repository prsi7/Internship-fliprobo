{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "martial-leadership",
   "metadata": {},
   "source": [
    "# Micro-Credit Defaulter Model\n",
    "Problem Statement: A Microfinance Institution (MFI) is an organization that offers financial services to low income populations. MFS becomes very useful when targeting especially the unbanked poor families living in remote areas with not much sources of income. The Microfinance services (MFS) provided by MFI are Group Loans, Agricultural Loans, Individual Business Loans and so on. Many microfinance institutions (MFI), experts and donors are supporting the idea of using mobile financial services (MFS) which they feel are more convenient and efficient, and cost saving, than the traditional high-touch model used since long for the purpose of delivering microfinance services. Though, the MFI industry is primarily focusing on low income families and are very useful in such areas, the implementation of MFS has been uneven with both significant challenges and successes. Today, microfinance is widely accepted as a poverty-reduction tool, representing $70 billion in outstanding loans and a global outreach of 200 million clients. We are working with one such client that is in Telecom Industry. They are a fixed wireless telecommunications network provider. They have launched various products and have developed its business and organization based on the budget operator model, offering better products at Lower Prices to all value conscious customers through a strategy of disruptive innovation that focuses on the subscriber. They understand the importance of communication and how it affects a person’s life, thus, focusing on providing their services and products to low income families and poor customers that can help them in the need of hour. They are collaborating with an MFI to provide micro-credit on mobile balances to be paid back in 5 days. The Consumer is believed to be defaulter if he deviates from the path of paying back the loaned amount within the time duration of 5 days. For the loan amount of 5 (in Indonesian Rupiah), payback amount should be 6 (in Indonesian Rupiah), while, for the loan amount of 10 (in Indonesian Rupiah), the payback amount should be 12 (in Indonesian Rupiah). The sample data is provided to us from our client database. It is hereby given to you for this exercise. In order to improve the selection of customers for the credit, the client wants some predictions that could help them in further investment and improvement in selection of customers. Exercise: Build a model which can be used to predict in terms of a probability for each loan transaction, whether the customer will be paying back the loaned amount within 5 days of insurance of loan. In this case, Label ‘1’ indicates that the loan has been payed i.e. Non- defaulter, while, Label ‘0’ indicates that the loan has not been payed i.e. defaulter.\n",
    "Points to Remember:\n",
    "\n",
    "There are no null values in the dataset.\n",
    "There may be some customers with no loan history.\n",
    "• The dataset is imbalanced. Label ‘1’ has approximately 87.5% records, while, label ‘0’ has approximately 12.5% records.\n",
    "• For some features, there may be values which might not be realistic. You may have to observe them and treat them with a suitable explanation.\n",
    "• You might come across outliers in some features which you need to handle as per your understanding. Keep in mind that data is expensive and we cannot lose more than 7-8% of the data.\n",
    "Find Enclosed the Data Description File and The Sample Data for the Modeling Exercise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "\n",
    "#Generic\n",
    "import numpy as np,pandas as pd, matplotlib.pyplot as plt, seaborn as sns, joblib\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "#Statistics\n",
    "from scipy.stats import zscore\n",
    "\n",
    "#Scaler\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "#Skewness\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "#Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Resample\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest,chi2,f_classif, VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "#Decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Hypertune Parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Classification Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "#Classification Metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,f1_score,accuracy_score,recall_score,precision_score\n",
    "from sklearn.metrics import auc,roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Desktop/QUERY/internship fliprobo/Micro-Credit-Project/Micro Credit Project/Data file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Delete first column of index\n",
    "df=df.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Duplicates\n",
    "df=df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Object type columns\n",
    "df.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop pcircle\n",
    "df=df.drop('pcircle',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change dtype of pdate to datetime64\n",
    "df['pdate']=pd.to_datetime(df['pdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check count of Label\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(df['label'],palette='Set2')\n",
    "plt.ylim(0,200000)\n",
    "plt.savefig('Desktop/QUERY/internship fliprobo/Micro-Credit-Project/Micro Credit Project//1.Unsampled_Label.jpeg',dpi=300)\n",
    "plt.show()\n",
    "#As we can see the label is highly imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot heatmap to check correlation\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.corr(),cmap='viridis')\n",
    "plt.savefig('Desktop/QUERY/internship fliprobo/Micro-Credit-Project/Micro Credit Project//2.Correlation_Heatmap.jpeg',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot barplot to check correlation\n",
    "plt.figure(figsize=(10,6))\n",
    "df.corr()['label'].drop('label').sort_values().plot(kind='bar')\n",
    "plt.savefig('Desktop/QUERY/internship fliprobo/Micro-Credit-Project/Micro Credit Project//3.Correlation_Barplot.jpeg',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='label',y='cnt_ma_rech30',data=df,palette='viridis')\n",
    "plt.ylim(0,20)\n",
    "plt.savefig('Desktop/QUERY/internship fliprobo/Micro-Credit-Project/Micro Credit Project//4.Box_Plot.jpeg',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check value counts of pdate\n",
    "df['pdate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract new column of month\n",
    "df['pmonth']=df['pdate'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need to extract year as there is only one distinct value\n",
    "df['pdate'].dt.year.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-preference",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Drop pdate column\n",
    "df=df.drop(['pdate'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('msisdn',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Skewness and Detect Outlier\n",
    "q1=df.quantile(q=0.25)\n",
    "q3=df.quantile(q=0.75)\n",
    "#Create IQR Range\n",
    "IQR=q3-q1\n",
    "lower_bound = q1 - 1.5*IQR\n",
    "higer_bound = q3 + 1.5*IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function for Outlier Detection\n",
    "def remove_outlier(df,col,inp):\n",
    "    if inp==False:\n",
    "        df_copy=df.copy()\n",
    "        raw=df_copy[col].shape[0]\n",
    "        df_copy=df_copy[(df_copy[col]>=lower_bound[col]) & (df_copy[col]<=higer_bound[col])]\n",
    "        prcsd=df_copy[col].shape[0]\n",
    "        \n",
    "        percent_change=(((raw-prcsd)/raw)*100)\n",
    "        outliers=raw-prcsd\n",
    "        \n",
    "        print(\"{} outliers are detected for column {} with percent change being {}\".format(outliers,col,percent_change))\n",
    "    elif inp==True:\n",
    "        raw=df[col].shape[0]\n",
    "        df=df[(df[col]>=lower_bound[col]) & (df[col]<=higer_bound[col])]\n",
    "        prcsd=df[col].shape[0]\n",
    "        \n",
    "        percent_change=(((raw-prcsd)/raw)*100)\n",
    "        outliers=raw-prcsd\n",
    "        \n",
    "        print(\"{} outliers are detected for column {} with percent change being {}\".format(outliers,col,percent_change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Function for each column\n",
    "for x in df.columns:\n",
    "    remove_outlier(df,str(x),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt=PowerTransformer()\n",
    "for x in df.columns.drop('label'):\n",
    "    if abs(df.loc[:,x].skew())>0.55:\n",
    "        df.loc[:,x]=pt.fit_transform(df.loc[:,x].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resample the data as it is highly imbalanced\n",
    "df_minority=df[df['label']==0]\n",
    "df_majority=df[df['label']==1]\n",
    "\n",
    "df_minority_upsampled=resample(df_minority,replace=True,n_samples=50000,random_state=101)\n",
    "\n",
    "df_upsampled=pd.concat([df_majority,df_minority_upsampled],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minority=df_upsampled[df_upsampled['label']==0]\n",
    "df_majority=df_upsampled[df_upsampled['label']==1]\n",
    "\n",
    "df_majority_downsampled=resample(df_majority,replace=False,n_samples=150000,random_state=101)\n",
    "\n",
    "df_downsampled=pd.concat([df_minority,df_majority_downsampled],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the data is expensive we cannot afford to loose more than 7-8% of data so we cannot directly downsample data to make the class balanced\n",
    "#Also, we cannot upsample the data from 25000 to 150000 as it would lead to many redundant data\n",
    "#Hence we have only downsampled 3-4% of data and we have upsampled that downsampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check resmapled data\n",
    "df_downsampled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check count of Label\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(df_downsampled['label'],palette='Set2')\n",
    "plt.ylim(0,200000)\n",
    "plt.savefig('Desktop/QUERY/internship fliprobo/Micro-Credit-Project/Micro Credit Project//5.Sampled_Data.jpeg',dpi=300)\n",
    "plt.show()\n",
    "#As we can see now the data is enough balanced to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df=pd.DataFrame()\n",
    "vif_df['Features']=df_downsampled.columns\n",
    "vif_df['VIF']=[variance_inflation_factor(df_downsampled.values,x) for x in range(len(df_downsampled.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df.sort_values(by='VIF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downsampled_copy=df_downsampled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downsampled_copy=df_downsampled_copy.drop(['medianamnt_loans30','medianamnt_ma_rech90','amnt_loans90','sumamnt_ma_rech30','sumamnt_ma_rech90',\n",
    "                         'cnt_ma_rech90','daily_decr90','amnt_loans30'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downsampled_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Scale data\n",
    "scale=MinMaxScaler()\n",
    "X=df_downsampled_copy.drop('label',axis=1)\n",
    "y=df_downsampled_copy['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=scale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Remove Features with variance less than 0.001\n",
    "# select=VarianceThreshold(threshold=0.001)\n",
    "# X=select.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Function which will evaluate model's performance\n",
    "model_list=[]\n",
    "score_list=[]\n",
    "def model_sel(mod):\n",
    "    model=mod\n",
    "    model.fit(X_train,y_train)\n",
    "    predict=model.predict(X_test)\n",
    "    f1score=f1_score(y_test,predict)\n",
    "    model_list.append(str(mod))\n",
    "    score_list.append(round(f1score,3))\n",
    "    print(\"****************** Metrics *********************\")\n",
    "    print()\n",
    "    print(\"Accuracy of the model is {}\".format(accuracy_score(y_test,predict)))\n",
    "    print(\"Recall of the model is {}\".format(recall_score(y_test,predict)))\n",
    "    print(\"Precision of the model is {}\".format(precision_score(y_test,predict)))\n",
    "    print(\"F1 score of the model is {}\".format(f1score))\n",
    "    print()\n",
    "    print(\"************** Confusion Matrix ****************\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_test,predict))\n",
    "    print()\n",
    "    print(\"*********** Classification Report **************\")\n",
    "    print()\n",
    "    print(classification_report(y_test,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run model for Logistic Regression\n",
    "model_sel(LogisticRegression(max_iter=3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Run Model for RandomForestClassifier\n",
    "model_sel(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Run Model for AdaBoostClassifier\n",
    "model_sel(AdaBoostClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Model for Support Vector Machines\n",
    "model_sel(SVC())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-alfred",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run Model for KNeighbors\n",
    "model_sel(KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create XGboost Dataset\n",
    "D_train = xgb.DMatrix(X_train, label=y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "#Create Parameters for XGboost\n",
    "param = {\n",
    "    'eta': 0.6, \n",
    "    'max_depth': 30,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3}\n",
    "steps = 80  # The number of training iterations\n",
    "\n",
    "#Train the model\n",
    "model = xgb.train(param, D_train, steps)\n",
    "\n",
    "#Perform Prediction\n",
    "preds = model.predict(D_test)\n",
    "\n",
    "#Choose best Prediction\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "print(\"****************** Metrics *********************\")\n",
    "print()\n",
    "print(\"Accuracy of the model is {}\".format(accuracy_score(y_test,best_preds)))\n",
    "print(\"Recall of the model is {}\".format(recall_score(y_test,best_preds)))\n",
    "print(\"Precision of the model is {}\".format(precision_score(y_test,best_preds)))\n",
    "print(\"F1 score of the model is {}\".format(f1_score(y_test,best_preds)))\n",
    "print()\n",
    "print(\"************** Confusion Matrix ****************\")\n",
    "print()\n",
    "print(confusion_matrix(y_test,best_preds))\n",
    "print()\n",
    "print(\"*********** Classification Report **************\")\n",
    "print()\n",
    "print(classification_report(y_test,best_preds))\n",
    "\n",
    "model_list.append('XGBoost')\n",
    "score_list.append(round(f1_score(y_test,best_preds),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(10,6))\n",
    "splot=sns.barplot(x=model_list,y=score_list,palette='twilight_r',tick_label=score_list,ax=ax)\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), '.3f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0.00, 9.00), \n",
    "                   textcoords = 'offset points')\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('F1Score')\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Images//6.Model_Performance.jpeg',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-villa",
   "metadata": {},
   "source": [
    "- From the above graph we can see that RandomForestClassifier is working very well\n",
    "- So we will try to hypertune its paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate object for RandomForest to optimize parameters\n",
    "model=RandomForestClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "predict=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cross_val_score(model,X_train,y_train,cv=4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,threshold=roc_curve(y_test,predict)\n",
    "auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(fpr,tpr,marker='o',markerfacecolor='red',markersize=10,linestyle='-.')\n",
    "plt.plot(threshold)\n",
    "plt.ylim(0,1.3)\n",
    "plt.xlim(0,1.3)\n",
    "plt.savefig('Images//7.AUC_ROC_Curve.jpeg',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypertune Parameter\n",
    "params={'n_estimators':[100,130,150,170,190,210,230,250,270,290,310,330]}\n",
    "gscv=GridSearchCV(model,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param=gscv.best_params_\n",
    "best_param['n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=RandomForestClassifier(n_estimators=best_param['n_estimators'])\n",
    "model.fit(X_train,y_train)\n",
    "predict=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"****************** Metrics *********************\")\n",
    "print()\n",
    "print(\"Accuracy of the model is {}\".format(accuracy_score(y_test,predict)))\n",
    "print(\"Recall of the model is {}\".format(recall_score(y_test,predict)))\n",
    "print(\"Precision of the model is {}\".format(precision_score(y_test,predict)))\n",
    "print(\"F1 score of the model is {}\".format(f1_score(y_test,predict)))\n",
    "print()\n",
    "print(\"************** Confusion Matrix ****************\")\n",
    "print()\n",
    "print(confusion_matrix(y_test,predict))\n",
    "print()\n",
    "print(\"*********** Classification Report **************\")\n",
    "print()\n",
    "print(classification_report(y_test,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict=pd.DataFrame(pd.Series(predict))\n",
    "df_test=pd.DataFrame(pd.Series(y_test))\n",
    "df_predict=pd.concat([df_predict.reset_index().drop('index',axis=1),df_test.reset_index().drop('index',axis=1)],axis=1)\n",
    "df_predict.columns=['Predicted','Original']\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_predict.loc[df_predict['Predicted']==df_predict['Original'],'Result']=True\n",
    "df_predict.loc[df_predict['Predicted']!=df_predict['Original'],'Result']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(df_predict['Result'],palette='twilight')\n",
    "plt.savefig('Images//8.Result.jpeg',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model,'Micro Credit Defaulter RF.obj') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= joblib.load('Micro Credit Defaulter RF.obj')\n",
    "y_pred = model.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
